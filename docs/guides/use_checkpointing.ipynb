{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9134fa",
   "metadata": {},
   "source": [
    "# Checkpointing with `flax.training.checkpoints`\n",
    "\n",
    "In this guide, you will learn about [`flax.training.checkpoints`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#module-flax.training.checkpoints)—a simplistic and generic checkpointing library built into Flax. With Flax Checkpoints, you can save and load model parameters, metadata, and a variety of Python data. In addition, it provides basic features for versioning, automatic bookkeeping of past checkpoints, and asynchronous saving to reduce training wait time.\n",
    "\n",
    "This guide covers the following:\n",
    "\n",
    "* Basic saving and loading of checkpoints with [`flax.training.checkpoints.save_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint).\n",
    "* More flexible and sustainable ways to load checkpoints ([`flax.training.checkpoints.restore_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.restore_checkpoint)).\n",
    "* How to save and load checkpoints when you run in multi-host scenarios with\n",
    "[`flax.training.checkpoints.save_checkpoint_multiprocess`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint_multiprocess).\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install/upgrade Flax, JAX, [Optax](https://optax.readthedocs.io/) and [TensorStore](https://google.github.io/tensorstore/). For JAX installation with GPU/TPU support, visit [this section on GitHub](https://github.com/google/jax#installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f8743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 185 kB 5.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 145 kB 77.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 44.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 237 kB 69.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 51 kB 5.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 85 kB 2.2 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U -q flax jax jaxlib optax tensorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-icO30rwmKYj",
   "metadata": {},
   "source": [
    "Note: Before running `import jax`, create eight fake devices to mimic multi-host checkpointing in this notebook. Note that the order of imports is important here. The `os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'` command works only with the CPU backend. This means it won't work with GPU/TPU acceleration on if you're running this notebook in Google Colab. If you are already running the code on multiple devices (for example, in a 4x2 TPU environment), you can skip running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ArKLnsyGRxGv",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SJT9DTxTytjn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from jax.experimental import maps, PartitionSpec, pjit\n",
    "from jax.experimental.array_serialization.serialization import GlobalAsyncCheckpointManager\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints, train_state\n",
    "from flax import struct, serialization\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d434cd",
   "metadata": {},
   "source": [
    "## Save checkpoints\n",
    "\n",
    "In Flax, you save and load any given JAX [pytree](https://jax.readthedocs.io/en/latest/pytrees.html) using the `flax.training.checkpoints` package. This includes not only typical Python and NumPy containers, but also customized classes extended from [`flax.struct.dataclass`](https://flax.readthedocs.io/en/latest/api_reference/flax.struct.html#flax.struct.dataclass). That means you can store almost any data generated—not only your model parameters, but any arrays/dictionaries, metadata/configs, and so on.\n",
    "\n",
    "Create a pytree with many data structures and containers, and play with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dec3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params=FrozenDict({\n",
       "     kernel: DeviceArray([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "                  [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "                  [ 0.36360955,  0.18376349, -0.68460613],\n",
       "                  [-0.8509373 , -0.64067173, -0.48081222],\n",
       "                  [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "     bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7f22297055f0>, update=<function chain.<locals>.update_fn at 0x7f2229705200>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [DeviceArray([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],            dtype=float32)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple model with one linear layer.\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x1 = random.normal(key1, (5,))      # A simple JAX array.\n",
    "model = nn.Dense(features=3)\n",
    "variables = model.init(key2, x1)\n",
    "\n",
    "# Flax's TrainState is a pytree dataclass and is supported in checkpointing.\n",
    "# Define your class with `@flax.struct.dataclass` decorator to make it compatible.\n",
    "tx = optax.sgd(learning_rate=0.1)      # An Optax SGD optimizer.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=tx)\n",
    "\n",
    "# Some arbitrary nested pytree with a dictionary, a string, and a NumPy array.\n",
    "config = {'dimensions': np.array([5, 3]), 'name': 'dense'}\n",
    "\n",
    "# Bundle everything together.\n",
    "ckpt = {'model': state, 'config': config, 'data': [x1]}\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc59dfa",
   "metadata": {},
   "source": [
    "Now save the checkpoint. You can add annotations like step number, prefix, and so on to your checkpoint.\n",
    "\n",
    "When saving a checkpoint, Flax will bookkeep the existing checkpoints based on your arguments. For example, by setting `overwrite=False` in [`flax.checkpoints.save_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint), Flax will not automatically save your checkpoint if there is already a step that is equal to or newer than the current one presently in the checkpoint directory. By setting `keep=2`, Flax will keep a maximum of 2 checkpoints in the directory. Learn more in the [API reference](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#module-flax.training.checkpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdb35ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tmp/flax-checkpointing/checkpoint_0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Flax Checkpoints.\n",
    "from flax.training import checkpoints\n",
    "\n",
    "from jax.experimental.array_serialization.serialization import GlobalAsyncCheckpointManager\n",
    "\n",
    "ckpt_dir = 'tmp/flax-checkpointing'\n",
    "\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)  # Remove any existing checkpoints from the last notebook run.\n",
    "\n",
    "checkpoints.save_checkpoint(ckpt_dir=ckpt_dir,\n",
    "                            target=ckpt,\n",
    "                            step=0,\n",
    "                            overwrite=False,\n",
    "                            keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b658bd1",
   "metadata": {},
   "source": [
    "## Restore checkpoints\n",
    "\n",
    "To restore a checkpoint, use [`flax.training.checkpoints.restore_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.restore_checkpoint) and pass in the checkpoint directory. Flax will automatically select the latest checkpoint in the directory. You can also choose to specify a step number or the path of the checkpoint file. You can always restore a pytree out of your checkpoints by setting `target=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150b20a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'step': 0,\n",
       "  'params': {'kernel': array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "          [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "          [ 0.36360955,  0.18376349, -0.68460613],\n",
       "          [-0.8509373 , -0.64067173, -0.48081222],\n",
       "          [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "   'bias': array([0., 0., 0.], dtype=float32)},\n",
       "  'opt_state': {'0': {}, '1': {}}},\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_restored = checkpoints.restore_checkpoint(ckpt_dir=ckpt_dir, target=None)\n",
    "raw_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b981f",
   "metadata": {},
   "source": [
    "However, when using `target=None`, the restored `raw_restored` will be different from the original `ckpt` in the following ways:\n",
    "\n",
    "1. There is no TrainState now, and only some raw weights and Optax state numbers remain.\n",
    "1. `metadata.dimensions` and `data` should be arrays, but restored as dictionaries with integers as keys.\n",
    "1. Previously, `data[0]` was a JAX NumPy array (`jnp.array`) —now it's a NumPy array (`numpy.array`).\n",
    "\n",
    "While (3) would not affect future work because JAX will automatically convert NumPy arrays to JAX arrays once the computation starts, (1) and (2) may lead to confusions.\n",
    "\n",
    "To resolve this, you should pass an example `target` in `flax.training.checkpoints.restore_checkpoint` to let Flax know exactly what structure it should restore to. The `target` should introduce any custom Flax dataclasses explicitly, and have the same structure as the saved checkpoint.\n",
    "\n",
    "It's often recommended to refactor out the process of initializing a checkpoint's structure (for example, a `TrainState`), so that saving/loading is easier and less error-prone. This is because complicated objects like `apply_fn` and `tx` are not stored in the checkpoint file and must be initiated by code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f42513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params={'kernel': array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "        [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "        [ 0.36360955,  0.18376349, -0.68460613],\n",
       "        [-0.8509373 , -0.64067173, -0.48081222],\n",
       "        [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32), 'bias': array([0., 0., 0.], dtype=float32)}, tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7f22297055f0>, update=<function chain.<locals>.update_fn at 0x7f2229705200>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=np.zeros_like(variables['params']),  # values of the tree leaf doesn't matter\n",
    "    tx=tx,\n",
    ")\n",
    "target = {'model': empty_state, 'config': None, 'data': [jnp.zeros_like(x1)]}\n",
    "state_restored = checkpoints.restore_checkpoint(ckpt_dir, target=target, step=0)\n",
    "state_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a300a",
   "metadata": {},
   "source": [
    "### Backward/forward dataclass compatibility\n",
    "\n",
    "The flexibility of using *Flax dataclasses*—[`flax.struct.dataclass`](https://flax.readthedocs.io/en/latest/api_reference/flax.struct.html#flax.struct.dataclass)—means that changes in Flax dataclass fields may break your existing checkpoints. For example, if you decide to add a field `batch_stats` to your `TrainState`, old checkpoints without this field may not be successfully restored. Same goes for removing a field in your dataclass.\n",
    "\n",
    "Note: Flax supports [`flax.struct.dataclass`](https://flax.readthedocs.io/en/latest/api_reference/flax.struct.html#flax.struct.dataclass), not Python's built-in `dataclasses.dataclass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be65d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError when target state has an unmentioned field:\n",
      "Missing field batch_stats in state dict while restoring an instance of CustomTrainState\n",
      "\n",
      "ValueError when target state misses a recorded field:\n",
      "Unknown field(s) \"batch_stats\" in state dict while restoring an instance of TrainState\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainState(train_state.TrainState):\n",
    "    batch_stats: Any = None\n",
    "\n",
    "custom_state = CustomTrainState.create(\n",
    "    apply_fn=state.apply_fn,\n",
    "    params=state.params,\n",
    "    tx=state.tx,\n",
    "    batch_stats=np.arange(10),\n",
    ")\n",
    "\n",
    "# Use a custom state to read the old `TrainState` checkpoint.\n",
    "custom_target = {'model': custom_state, 'config': None, 'data': [jnp.zeros_like(x1)]}\n",
    "try:\n",
    "    checkpoints.restore_checkpoint(ckpt_dir, target=custom_target, step=0)\n",
    "except ValueError as e:\n",
    "    print('ValueError when target state has an unmentioned field:')\n",
    "    print(e)\n",
    "    print('')\n",
    "\n",
    "\n",
    "# Use the old `TrainState` to read the custom state checkpoint.\n",
    "custom_ckpt = {'model': custom_state, 'config': config, 'data': [x1]}\n",
    "checkpoints.save_checkpoint(ckpt_dir, custom_ckpt, step=1, overwrite=True, keep=2)\n",
    "try:\n",
    "    checkpoints.restore_checkpoint(ckpt_dir, target=target, step=1)\n",
    "except ValueError as e:\n",
    "    print('ValueError when target state misses a recorded field:')\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c2255",
   "metadata": {},
   "source": [
    "It is recommended to keep your checkpoints up to date with your pytree dataclass definitions. You can keep a copy of your code along with your checkpoints.\n",
    "\n",
    "But if you must restore checkpoints and Flax dataclasses with incompatible fields, you can manually add/remove corresponding fields before passing in the correct target structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29fd1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': CustomTrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params=FrozenDict({\n",
       "     kernel: array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "            [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "            [ 0.36360955,  0.18376349, -0.68460613],\n",
       "            [-0.8509373 , -0.64067173, -0.48081222],\n",
       "            [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "     bias: array([0., 0., 0.], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7f22297055f0>, update=<function chain.<locals>.update_fn at 0x7f2229705200>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass no target to get a raw state dictionary first.\n",
    "raw_state_dict = checkpoints.restore_checkpoint(ckpt_dir, target=None, step=0)\n",
    "# Add/remove fields as needed.\n",
    "raw_state_dict['model']['batch_stats'] = np.arange(10)\n",
    "# Restore the classes with correct target now\n",
    "serialization.from_state_dict(custom_target, raw_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b39501",
   "metadata": {},
   "source": [
    "## Asynchronized checkpointing\n",
    "\n",
    "Checkpointing is I/O heavy, and if you have a large amount of data to save, it may be worthwhile to put it into a background thread, while continuing with your training.\n",
    "\n",
    "You can do this by creating an `async_manager` (as demonstrated in the code cell below) and let it track your save thread.\n",
    "\n",
    "`async_manager` is a parameter in [`flax.training.checkpoints.save_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint) with the default setting at `None`.\n",
    "\n",
    "Note that you should use the same `async_manager` to handle all your async saves across your training steps, so that it can make sure that a previous async save is done before the next one begins. This allows bookkeeping like `keep` and `overwrite` to be consistent across steps.\n",
    "\n",
    "Whenever you want to explicitly wait until an async save is done, you can call `async_manager.wait_previous_save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85be68a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The previous async save_checkpoint has not finished yet. Waiting for it to complete before the next save.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': {'step': 0,\n",
       "  'params': {'kernel': array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "          [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "          [ 0.36360955,  0.18376349, -0.68460613],\n",
       "          [-0.8509373 , -0.64067173, -0.48081222],\n",
       "          [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "   'bias': array([0., 0., 0.], dtype=float32)},\n",
       "  'opt_state': {'0': {}, '1': {}}},\n",
       " 'config': {'dimensions': array([5, 3]), 'name': 'dense'},\n",
       " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am = checkpoints.AsyncManager()\n",
    "\n",
    "# Mimic a training loop here:\n",
    "for step in range(2, 3):\n",
    "    checkpoints.save_checkpoint(ckpt_dir, ckpt, step=2, overwrite=True, keep=3, async_manager=am)\n",
    "    # ... Continue with your work...\n",
    "\n",
    "# ... Until a time when you want to wait until the save completes:\n",
    "am.wait_previous_save()  # Block until the checkpoint saving is completed.\n",
    "checkpoints.restore_checkpoint(ckpt_dir, target=None, step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e93db6",
   "metadata": {},
   "source": [
    "## Multi-host/multi-process checkpointing\n",
    "\n",
    "JAX provides a few ways to scale up your code on multiple hosts at the same time. This usually happens when the number of devices (CPU/GPU/TPU) is so large that different devices are managed by different hosts (CPU). To get started on JAX in multi-process settings, check out [Using JAX in multi-host and multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html).\n",
    "\n",
    "In the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm with JAX [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html), a large multi-process array can have its data sharded across different devices (check out the `pjit` [JAX-101 tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html)). This kind of data array needs a special experimental JAX API—[`GlobalAsyncCheckpointManager`](https://github.com/google/jax/blob/c7dcd0913bc8b4878a7e45184553c331254b801a/jax/experimental/gdarray_serializationerialization.py#L452)—to save and restore checkpoints. This API lets each host dump its data shards to a single shared storage, such as a Google Cloud bucket.\n",
    "\n",
    "Flax provides an easy interface for users to pass in a `GlobalAsyncCheckpointManager` and store pytrees with multi-process arrays in the same fashion as single-process pytrees. Just use [`flax.training.checkpoints.save_checkpoint_multiprocess`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint_multiprocess) with the same arguments.\n",
    "\n",
    "Unfortunately, Python Jupyter notebooks are single-host only and cannot activate the multi-host mode. As a workaround, use the following code as a sample to run your multi-host checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d199c8fa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a checkpoint with a multi-process array.\n",
    "\n",
    "# In reality, you should set this with multiple num_processes.\n",
    "# Refer to https://jax.readthedocs.io/en/latest/multi_process.html#initializing-the-cluster\n",
    "jax.distributed.initialize(\"localhost:8889\", num_processes=1, process_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ubdUvyMrhD-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-process array.\n",
    "mesh_shape = (4, 2)\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = maps.Mesh(devices, ('x', 'y'))\n",
    "\n",
    "f = pjit.pjit(\n",
    "  lambda x: x,\n",
    "  in_axis_resources=None,\n",
    "  out_axis_resources=PartitionSpec('x', 'y'))\n",
    "\n",
    "with maps.Mesh(mesh.devices, mesh.axis_names):\n",
    "    mp_array = f(np.arange(8 * 2).reshape(8, 2))\n",
    "\n",
    "# Make it a pytree as usual.\n",
    "mp_ckpt = {'model': mp_array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc355ce",
   "metadata": {},
   "source": [
    "### Example: Save a checkpoint in a multi-process setting with `save_checkpoint_multiprocess`\n",
    "\n",
    "The arguments in [`flax.training.checkpoints.save_checkpoint_multiprocess`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint_multiprocess) are the same as in [`flax.training.checkpoints.save_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.save_checkpoint), except for the additional `gda_manager` argument.\n",
    "\n",
    "If your checkpoint is too large, you can specify `timeout_secs` in the manager and give it more time to finish writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d10039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tmp/flax-checkpointing/checkpoint_3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gacm = GlobalAsyncCheckpointManager(timeout_secs=50)\n",
    "checkpoints.save_checkpoint_multiprocess(ckpt_dir, mp_ckpt, step=3, overwrite=True, \n",
    "                                         keep=4, gda_manager=gacm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d954c3c7",
   "metadata": {},
   "source": [
    "### Example: Restoring a checkpoint with `flax.training.checkpoints.restore_checkpoint`\n",
    "\n",
    "Note that, when using [`flax.training.checkpoints.restore_checkpoint`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.checkpoints.restore_checkpoint), you need to pass a `target` with valid multi-process arrays at the correct structural location. Flax only uses the `target` arrays' meshes and mesh axes to restore the checkpoint. This means that the multi-process array in the `target` arg doesn't have to be as large as your checkpoint's size (the shape of the multi-process array doesn't need to have the same shape as the actual array in your checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f9724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': array([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]], dtype=int32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with maps.Mesh(mesh.devices, mesh.axis_names):\n",
    "    mp_smaller_array = f(np.zeros(8).reshape(4, 2))\n",
    "\n",
    "mp_target = {'model': mp_smaller_array}\n",
    "mp_restored = checkpoints.restore_checkpoint(ckpt_dir, target=mp_target, \n",
    "                                             step=3, gda_manager=gacm)\n",
    "mp_restored"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
