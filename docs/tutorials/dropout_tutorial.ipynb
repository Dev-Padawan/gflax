{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ywkAn3BtzBiu"
   },
   "outputs": [],
   "source": [
    "# Image classification with a dropout and an optimizer schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY6InV4qzAQG"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/tutorials/dropout_tutorial.ipynb)\n",
    "\n",
    "This tutorial provides an end-to-end example of a simple image classification model with a Flax dropout layer Flax and an [Optax](https://optax.readthedocs.io/) optimizer learning rate schedule. The [dropout](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) stochastic regularization technique randomly removes hidden and visible units in a network. Learning rate scheduling helps modulate how the learning rate of the optimizer changes over time.\n",
    "\n",
    "- The randomness in Flax's [`Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout) layer is handled internally with [`flax.linen.Module.make_rng`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#flax.linen.Module.make_rng). This is covered in more detail in [ðŸ”ª Flax - The Sharp Bits ðŸ”ª `flax.linen.Dropout` layer and randomness](https://flax.readthedocs.io/en/latest/notebooks/flax_sharp_bits.html#flax-linen-dropout-layer-and-randomness).\n",
    "- This tutorial uses the [`optax.linear_schedule`](https://optax.readthedocs.io/en/latest/api.html#optax.linear_schedule) optimizer schedule. For a full list of optimizer schedules, check out the [Optax API docs](https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules).\n",
    "\n",
    "This training example uses a lot of fundamental concepts covered in [Getting started](https://flax.readthedocs.io/en/latest/getting_started.html). If you're new to Flax, start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpnTy21qSWYV"
   },
   "source": [
    "## Setup\n",
    "\n",
    "- Install/upgrade Flax, which will also set up [Optax](https://optax.readthedocs.io/) (for common optimizers, loss functions, and optimizer schedules), and JAX.\n",
    "- Install [TensorFlow Datasets](https://www.tensorflow.org/datasets) to load a dataset for this tutorial.\n",
    "- Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4Q1sL4cQTbAt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 189 kB 5.0 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 40.8 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154 kB 57.2 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.3 MB 39.7 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 237 kB 5.1 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 2.6 MB/s \n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85 kB 1.6 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q flax tensorflow_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1kYKJYxxXUiL"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Flax Linen API\n",
    "from flax.training import train_state  # A Flax dataclass to keep the train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # The Optax library\n",
    "import tensorflow_datasets as tfds     # TFDS for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTf7YtdgWWNY"
   },
   "source": [
    "Use a JAX PRNG key and split it to get one key for parameter initialization, and another one for dropout randomness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G9lWcsK1WUzL"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "root_key = jax.random.PRNGKey(seed=seed)\n",
    "main_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9pSTAIOSU1X"
   },
   "source": [
    "Create a simple Flax model, subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics). Note that:\n",
    "\n",
    "- To add a dropout layer in Flax, use [`flax.linen.Dropout`](https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Dropout.html#flax.linen.Dropout).\n",
    "- In `flax.linen.Dropout`, the `deterministic` argument is `None` by default. If `false`, the inputs are scaled by `1 / (1 - dropout_rate)` and masked. When it's `true`, no mask is applied (the dropout is turned off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yNWL1sDOZpTj"
   },
   "outputs": [],
   "source": [
    "# A simple convolutional network with a dropout layer.\n",
    "class CNN(nn.Module):\n",
    "  training: bool\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # Flatten\n",
    "    # Set the dropout layer with a rate of 50% .\n",
    "    # When the `deterministic` flag is `True`, dropout is turned off.\n",
    "    x = nn.Dropout(rate=0.5, deterministic=not self.training)(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk-E94kkT51x"
   },
   "source": [
    "Define the loss function using [Optax](https://optax.readthedocs.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HXUHeCptgRXF"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, logits, labels):\n",
    "  labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "  return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDOYyP4uT-Wt"
   },
   "source": [
    "Create a function for the loss and accuracy metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-IkOdtakLok5"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL8F_kN5UAg1"
   },
   "source": [
    "Write a function for loading your dataset with [TensorFlow Datasets](https://www.tensorflow.org/datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "t9sGGsSzLx13"
   },
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "  ds_builder = tfds.builder('mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "  train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "  test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEiC2gBXUHlS"
   },
   "source": [
    "Create another function for creating the Flax [`TrainState`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#train-state) with an [Optax](https://optax.readthedocs.io/) optimizer.\n",
    "\n",
    "Remember that:\n",
    "\n",
    "- When initializing the variables, use the `params_key` PRNG key (the `params_key` is equivalent to a dictionary of PRNGs).\n",
    "- The model constructor is `training=False` before you start training.\n",
    "\n",
    "This example uses the AdaBelief optimizer ([`optax.adabelief`](https://optax.readthedocs.io/en/latest/api.html#adabelief))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aae3rz125R6z"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate):\n",
    "  # Instantiate the model with `training=False`.\n",
    "  cnn = CNN(training=False)\n",
    "  # Initialize the `params`. Use the `params_key` PRNG key.\n",
    "  # (Here, you are providing only one PRNG key.) \n",
    "  params = cnn.init(params_key, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  # Use the Optax optimizer.\n",
    "  # The `learning_rate_fn` is an Optax learning rate schedule (defined further below).\n",
    "  tx = optax.adabelief(learning_rate_fn)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRDfpGlkV5m5"
   },
   "source": [
    "Define the training step function. Note that:\n",
    "\n",
    "- During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply), use the `'dropout'` key (`dropout_key`) for the `rngs` argument.\n",
    "- The model constructor argument should be set to `training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CPsXnjKVH0hV"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    # Perform the forward pass with `flax.linen.apply()`.\n",
    "    # Use the `dropout_key` for the `rngs` argument.\n",
    "    logits = CNN(training=True).apply({'params': params}, batch['image'], rngs={'dropout': dropout_key})\n",
    "    # Calculate the loss,\n",
    "    loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "    return loss, logits\n",
    "  # Compute the gradients\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, logits = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "  return state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN5u62E6V8kJ"
   },
   "source": [
    "Write the evaluation step function. Remember to set the model constructor argument to `training=false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IKODKHNNH_1k"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "  logits = CNN(training=False).apply({'params': params}, batch['image'])\n",
    "  return compute_metrics(logits=logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dio74TEWBjt"
   },
   "source": [
    "Create a function for training the model for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6dY_lKUz5SuG"
   },
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, train_ds_size)\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # Skip an incomplete batch.\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "  batch_metrics = []\n",
    "  for perm in perms:\n",
    "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "    state, metrics = train_step(state, batch)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  # Compute the mean of metrics across each batch in an epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]\n",
    "      }\n",
    "\n",
    "  print('Train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
    "      epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "  return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuX4Ewz3hMZM"
   },
   "source": [
    "Create a model evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Sq5z385KPTuu"
   },
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "  metrics = eval_step(params, test_ds)\n",
    "  metrics = jax.device_get(metrics)\n",
    "  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics)\n",
    "  return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Ij6G7jw8xc"
   },
   "source": [
    "Download the dataset and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P5JcAhIs5Szz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d3979925954539a3bdba971c084585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Dle_aEuqBc9"
   },
   "source": [
    "Define a learning rate schedule. In this example, use [`optax.linear_schedule`](https://optax.readthedocs.io/en/latest/api.html#optax.linear_schedule).\n",
    "\n",
    "Note: You can learn more about Optax, its optimizers, loss functions and schedules in the [Optax tutorial](https://optax.readthedocs.io/en/latest/optax-101.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u90ieN6VnWQC"
   },
   "outputs": [],
   "source": [
    "def create_learning_rate_fn(base_learning_rate, end_learning_rate, warmup_steps):\n",
    "  warmup_fn = optax.linear_schedule(\n",
    "      init_value=base_learning_rate,\n",
    "      end_value=end_learning_rate,\n",
    "      transition_steps=warmup_steps,\n",
    "  )\n",
    "  decay_fn = optax.linear_schedule(\n",
    "      init_value=end_learning_rate,\n",
    "      end_value=base_learning_rate,\n",
    "      transition_steps=train_steps - warmup_steps,\n",
    "  )\n",
    "  schedule_fn = optax.join_schedules(\n",
    "      schedules=[warmup_fn, decay_fn],\n",
    "      boundaries=[warmup_steps],\n",
    "  )\n",
    "\n",
    "  return schedule_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFKcPCb8qZYY"
   },
   "source": [
    "Instantiate the learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gPaGFeWoqAbF"
   },
   "outputs": [],
   "source": [
    "train_steps = 10\n",
    "base_learning_rate = 0.0001\n",
    "end_learning_rate = 0.001\n",
    "warmup_steps = 0.0\n",
    "\n",
    "learning_rate_fn = create_learning_rate_fn(\n",
    "    base_learning_rate=base_learning_rate,\n",
    "    end_learning_rate=end_learning_rate,\n",
    "    warmup_steps=warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDfng16c5S31"
   },
   "source": [
    "Initialize the Flax `TrainState`, passing in the learning rate schedule for the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qrLNat6c5S7Z"
   },
   "outputs": [],
   "source": [
    "state = create_train_state(rng=params_key, learning_rate=learning_rate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b66WaNi8xBHz"
   },
   "source": [
    "Train the model over 5 epochs, and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BpSx0ZBV5S-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1, loss: 0.4546, accuracy: 87.19\n",
      "  Test epoch: 1, loss: 0.22, accuracy: 93.53\n",
      "Train epoch: 2, loss: 0.1949, accuracy: 94.38\n",
      "  Test epoch: 2, loss: 0.15, accuracy: 95.87\n",
      "Train epoch: 3, loss: 0.1407, accuracy: 95.88\n",
      "  Test epoch: 3, loss: 0.11, accuracy: 96.86\n",
      "Train epoch: 4, loss: 0.1119, accuracy: 96.74\n",
      "  Test epoch: 4, loss: 0.09, accuracy: 97.28\n",
      "Train epoch: 5, loss: 0.0935, accuracy: 97.26\n",
      "  Test epoch: 5, loss: 0.08, accuracy: 97.71\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "  # Use a separate PRNG key to permute image data during shuffling.\n",
    "  main_key, input_rng = jax.random.split(key=root_key)\n",
    "  # Run an optimization step over a training batch.\n",
    "  state = train_epoch(state=state,\n",
    "                      train_ds=train_ds,\n",
    "                      batch_size=batch_size,\n",
    "                      epoch=epoch,\n",
    "                      rng=input_rng)\n",
    "  # Evaluate on the test set after each training epoch.\n",
    "  test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "  print('  Test epoch: %d, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, test_loss, test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dropout_tutorial.ipynb",
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
