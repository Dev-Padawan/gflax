# FLAX Testimonials

## Thomas Kipf

I am in the final stages of finishing my PhD in Machine Learning at the University of Amsterdam, where I worked on developing some of the first graph neural network models (e.g. [graph convolutional networks](https://arxiv.org/abs/1609.02907)) before joining the Brain Team in Google Research earlier this year. In the past five years, I have been using various frameworks such as Theano, TensorFlow, and PyTorch. More recently, I have become a big fan of JAXâ€™s simplicity and speed, and thanks to FLAX we now have a flexible and lightweight library for building neural nets (and graph neural nets!) in the JAX ecosystem.

## Luke Metz

My goal has been to find more real models to test optimizers on. Many examples that I found either only work on open source, or are a pile of abstraction and hard to modify, or buggy... The Flax examples just worked. So thank you! I did all this in one day and had never touched Flax before! Was pretty great all and all though I really didn't need to do much".

## Bernd Bohnet

I've been working at Google for five years, and before that I was working for over 10 years in academica. My area of expertise is NLP. I used perceptrons and SVMs for structural learning and ML frameworks such as DyNet and Tensorflow. I used FLAX for 3 NLP projects now, and it is very easy to work with, it is easy to read and understand. It is not over-engineered, which makes it easy to implement things quickly.

## Joost Bastings

I did a PhD on Neural Machine Translation and Machine Learning for NLP, and I joined Google in October 2019. In the last 5 years I've been using many different frameworks such as Theano, Tensorflow, PyTorch, and now FLAX. Generally frameworks got better over time. For example, Tensorflow provided sparse matrix multiplication useful for graph convolutional neural networks, and PyTorch made dynamic network structures easy to implement, e.g. Tree LSTMs. What I like best about FLAX is that it allows for readable, yet high performance code.

## Marc van Zee

I have been working as a SWE on C++ for about 3 years at Google. I started working on machine learning about a year ago, and I have mostly worked with the tensor2tensor framework. In FLAX, it took me a while to understand how it works, but it has given me a very deep understanding of how machine learning algorithms work. Also the community around FLAX is great, it is very collaborative and open.
