{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9WN5UZzRqkE"
   },
   "source": [
    "# Variational autoencoder with a learning rate schedule\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/docs/tutorials/vae_tutorial.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to train a simple variational autoencoder (VAE) end-to-end with learning rate scheduling using Flax and [Optax](https://optax.readthedocs.io/).\n",
    "\n",
    "For the optimizer schedule, you will use the linear warmup followed by cosine decay ([`optax.warmup_cosine_decay_schedule`](https://optax.readthedocs.io/en/latest/api.html#optax.warmup_cosine_decay_schedule)).\n",
    "\n",
    "The tutorial uses a lot of fundamental concepts covered in [Getting started](https://flax.readthedocs.io/en/latest/getting_started.html). If you're new to Flax, start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpnTy21qSWYV"
   },
   "source": [
    "## Setup\n",
    "\n",
    "- Install/upgrade Flax, which will also set up [Optax](https://optax.readthedocs.io/) (for common optimizers and loss functions), and JAX.\n",
    "- Install [TensorFlow Datasets](https://www.tensorflow.org/datasets) to load a dataset for this tutorial.\n",
    "- Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4Q1sL4cQTbAt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 189 kB 10.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 56.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 8.3 MB 16.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 237 kB 54.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 154 kB 63.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 51 kB 4.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 85 kB 2.6 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q flax tensorflow_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1kYKJYxxXUiL"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp                # JAX NumPy\n",
    "\n",
    "from flax import linen as nn           # The Flax Linen API\n",
    "from flax.training import train_state  # A Flax dataclass to keep the train state\n",
    "\n",
    "import numpy as np                     # Ordinary NumPy\n",
    "import optax                           # The Optax library\n",
    "import tensorflow as tf                # TensorFlow for preprocessing operations (`tf.cast`, `tf.reshape`)\n",
    "import tensorflow_datasets as tfds     # TFDS for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9pSTAIOSU1X"
   },
   "source": [
    "Create a simple [variational autoencoder](https://arxiv.org/abs/1312.6114) model, subclassed from [Flax `Module`](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#module-basics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "24sGnMGXYBaU"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  latents: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(500, name='fc1')(x)\n",
    "    x = nn.relu(x)\n",
    "    mean_x = nn.Dense(self.latents, name='fc2_mean')(x)\n",
    "    logvar_x = nn.Dense(self.latents, name='fc2_logvar')(x)\n",
    "    return mean_x, logvar_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rPnXF1LRYqan"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, z):\n",
    "    z = nn.Dense(500, name='fc1')(z)\n",
    "    z = nn.relu(z)\n",
    "    z = nn.Dense(784, name='fc2')(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-IahsXnifllL"
   },
   "outputs": [],
   "source": [
    "def reparameterize(rng, mean, logvar):\n",
    "  std = jnp.exp(0.5 * logvar)\n",
    "  eps = jax.random.normal(rng, logvar.shape)\n",
    "  return mean + eps * std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9pOD0G49e593"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "  latents: int = 20\n",
    "\n",
    "  def setup(self):\n",
    "    self.encoder = Encoder(self.latents)\n",
    "    self.decoder = Decoder()\n",
    "\n",
    "  def __call__(self, x, z_rng):\n",
    "    mean, logvar = self.encoder(x)\n",
    "    z = reparameterize(z_rng, mean, logvar)\n",
    "    recon_x = self.decoder(z)\n",
    "    return recon_x, mean, logvar\n",
    "\n",
    "  def generate(self, z):\n",
    "    return nn.sigmoid(self.decoder(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ewLjblgq5mQC"
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "  return VAE(latents=latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk-E94kkT51x"
   },
   "source": [
    "Define the binary cross-entropy loss function.\n",
    "\n",
    "In addition to optimizers, Optax provides a number of common loss functions, including [`optax.sigmoid_binary_cross_entropy`](https://optax.readthedocs.io/en/latest/api.html#optax.sigmoid_binary_cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HXUHeCptgRXF"
   },
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def binary_cross_entropy_with_logits(logits, labels):\n",
    " logits = nn.log_sigmoid(logits)\n",
    " return -jnp.sum(labels * logits + (1. - labels) * jnp.log(-jnp.expm1(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arucT1uPf00j"
   },
   "source": [
    "Define the KL divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g4F9g-Frf352"
   },
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def kl_divergence(mean, logvar):\n",
    "  return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDOYyP4uT-Wt"
   },
   "source": [
    "Create a function for the loss and accuracy metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-IkOdtakLok5"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(recon_x, x, mean, logvar):\n",
    "  bce_loss = binary_cross_entropy_with_logits(recon_x, x).mean()\n",
    "  kld_loss = kl_divergence(mean, logvar).mean()\n",
    "  return {\n",
    "      'bce': bce_loss,\n",
    "      'kld': kld_loss,\n",
    "      'loss': bce_loss + kld_loss\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRDfpGlkV5m5"
   },
   "source": [
    "Define the training step function. Note that:\n",
    "\n",
    "- During the forward pass with [`flax.linen.apply()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html#init-apply).\n",
    "- The model constructor argument should be set to `training=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CPsXnjKVH0hV"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch, z_rng):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    # Perform the forward pass with `flax.linen.apply()`.\n",
    "    recon_x, mean, logvar = model().apply({'params': params}, batch, z_rng)\n",
    "    # Calculate the binary cross-entropy loss.\n",
    "    bce_loss = binary_cross_entropy_with_logits(recon_x, batch).mean()\n",
    "    # Calculate the KL divergence loss.\n",
    "    kld_loss = kl_divergence(mean, logvar).mean()\n",
    "    # Calculate the total loss.\n",
    "    loss = bce_loss + kld_loss\n",
    "    return loss\n",
    "  # Compute the gradients\n",
    "  grads = jax.grad(loss_fn)(state.params)\n",
    "  return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN5u62E6V8kJ"
   },
   "source": [
    "Write the evaluation step function. Remember to set the model constructor argument to `training=false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IKODKHNNH_1k"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, images, z, z_rng):\n",
    "  def eval_model(vae):\n",
    "    recon_images, mean, logvar = vae(images, z_rng)\n",
    "    comparison = jnp.concatenate([images[:8].reshape(-1, 28, 28, 1),\n",
    "                                  recon_images[:8].reshape(-1, 28, 28, 1)])\n",
    "\n",
    "    generate_images = vae.generate(z)\n",
    "    generate_images = generate_images.reshape(-1, 28, 28, 1)\n",
    "    metrics = compute_metrics(recon_images, images, mean, logvar)\n",
    "    return metrics, comparison, generate_images\n",
    "\n",
    "  return nn.apply(eval_model, model())({'params': params})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Ij6G7jw8xc"
   },
   "source": [
    "Download the dataset and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "421NeRsFue4K"
   },
   "outputs": [],
   "source": [
    "def prepare_image(x):\n",
    "  x = tf.cast(x['image'], tf.float32)\n",
    "  x = tf.reshape(x, (-1,))\n",
    "  return x\n",
    "\n",
    "# Write a function for loading your dataset with [TensorFlow Datasets](https://www.tensorflow.org/datasets):\n",
    "def get_datasets():\n",
    "  ds_builder = tfds.builder('binarized_mnist')\n",
    "  ds_builder.download_and_prepare()\n",
    "  train_ds = ds_builder.as_dataset(split=tfds.Split.TRAIN)\n",
    "  train_ds = train_ds.map(prepare_image)\n",
    "  train_ds = train_ds.cache()\n",
    "  train_ds = train_ds.repeat()\n",
    "  train_ds = train_ds.shuffle(50000)\n",
    "  train_ds = train_ds.batch(batch_size)\n",
    "  train_ds = iter(tfds.as_numpy(train_ds))\n",
    "\n",
    "  test_ds = ds_builder.as_dataset(split=tfds.Split.TEST)\n",
    "  test_ds = test_ds.map(prepare_image).batch(10000)\n",
    "  test_ds = np.array(list(test_ds)[0])\n",
    "  test_ds = jax.device_put(test_ds)\n",
    "\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t65bqn3E7xPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 104.68 MiB (download: 104.68 MiB, generated: Unknown size, total: 104.68 MiB) to /root/tensorflow_datasets/binarized_mnist/1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984cc7334c65428d95d17f1b9de1f82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf917fe80164ae19b4101d652c5655a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170317c1cf2941939ecdc8ce78c343e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8ce10936484ea78e52a5a6d351310d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72de56ba644cdd9711baaa5bd91b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/binarized_mnist/1.0.0.incompleteBBC09W/binarized_mnist-train.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daae32618a63448997a97921d279db79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c55e0a1420840a09beaa4841e47e31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/binarized_mnist/1.0.0.incompleteBBC09W/binarized_mnist-validation.tfrecord…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92abe70ce0c843e49629309ef0e599cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9130ce96f1ff4b9cab3f16a6dcb61a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/binarized_mnist/1.0.0.incompleteBBC09W/binarized_mnist-test.tfrecord*...: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset binarized_mnist downloaded and prepared to /root/tensorflow_datasets/binarized_mnist/1.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrjHKpNfj4bw"
   },
   "source": [
    "Use a JAX PRNG key and split it to get one key for parameter initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "itqf1Acoh5HO"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = jax.random.PRNGKey(seed=seed)\n",
    "rng, key = jax.random.split(key=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ig8yKN7p1TuC"
   },
   "outputs": [],
   "source": [
    "latents = 20\n",
    "\n",
    "rng, z_key, eval_rng = jax.random.split(rng, 3)\n",
    "z = jax.random.normal(z_key, (64, latents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDfng16c5S31"
   },
   "source": [
    "Create and initialize the Flax [`TrainState`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#train-state) with an [Optax](https://optax.readthedocs.io/) optimizer. Remember that:\n",
    "\n",
    "- When initializing the variables, use the `params_key` PRNG key (the `params_key` is equivalent to a dictionary of PRNGs).\n",
    "- The model constructor is `training=False` before you start training.\n",
    "\n",
    "This example uses the Yogi optimizer ([`optax.adabelief`](https://optax.readthedocs.io/en/latest/api.html#yogi))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qrLNat6c5S7Z"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate_fn):\n",
    "  # Instantiate the model with `training=False`.\n",
    "  init_data = jnp.ones((batch_size, 784), jnp.float32)\n",
    "  params = model().init(key, init_data, rng)['params']\n",
    "  # Use an Optax optimizer.\n",
    "  # The `learning_rate_fn` is an Optax learning rate schedule (defined further below).\n",
    "  tx = optax.yogi(learning_rate_fn)\n",
    "  return train_state.TrainState.create(\n",
    "      apply_fn=model().apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSffwrxFKZso"
   },
   "source": [
    "Define a learning rate schedule that uses [`optax.linear_schedule`](https://optax.readthedocs.io/en/latest/api.html#optax.linear_schedule) and [`optax.cosine_decay_schedule`](https://optax.readthedocs.io/en/latest/api.html#optax.cosine_decay_schedule) (cosine learning rate decay).\n",
    "\n",
    "Note: You can learn more about Optax, its optimizers, loss functions and schedules in the [Optax tutorial](https://optax.readthedocs.io/en/latest/optax-101.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LWM0mJYR7jsJ"
   },
   "outputs": [],
   "source": [
    "def create_learning_rate_fn(base_learning_rate, warmup_steps, steps_per_epoch):\n",
    "  warmup_fn = optax.linear_schedule(\n",
    "      init_value=0.0,\n",
    "      end_value=base_learning_rate,\n",
    "      transition_steps=warmup_steps * steps_per_epoch,\n",
    "  )\n",
    "  cosine_epochs = max(num_epochs - warmup_steps, 1)\n",
    "  cosine_fn = optax.cosine_decay_schedule(\n",
    "      init_value=base_learning_rate,\n",
    "      decay_steps=cosine_epochs * steps_per_epoch\n",
    "  )\n",
    "  schedule_fn = optax.join_schedules(\n",
    "      schedules=[warmup_fn, cosine_fn],\n",
    "      boundaries=[warmup_steps, steps_per_epoch],\n",
    "  )\n",
    "\n",
    "  return schedule_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWxcjUaDKzOj"
   },
   "source": [
    "Instantiate the learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r-QJVoan9vNw"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10 # For simplicity, train for 10 epochs.\n",
    "base_learning_rate = 0.0001\n",
    "warmup_steps = 1.0\n",
    "steps_per_epoch = 10 \n",
    "\n",
    "learning_rate_fn = create_learning_rate_fn(\n",
    "    base_learning_rate=base_learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    steps_per_epoch=steps_per_epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROGRDeMdK58v"
   },
   "source": [
    "Initialize the Flax `TrainState`, passing in the learning rate schedule for the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wa8dsLoJ95na"
   },
   "outputs": [],
   "source": [
    "state = create_train_state(rng=key, learning_rate_fn=learning_rate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b66WaNi8xBHz"
   },
   "source": [
    "Train the model for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BpSx0ZBV5S-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval epoch: 1, loss: 542.03, binary cross-entropy: 541.22, KL divergence: 0.81\n",
      "Eval epoch: 2, loss: 496.17, binary cross-entropy: 494.80, KL divergence: 1.37\n",
      "Eval epoch: 3, loss: 457.73, binary cross-entropy: 455.39, KL divergence: 2.34\n",
      "Eval epoch: 4, loss: 422.83, binary cross-entropy: 418.90, KL divergence: 3.93\n",
      "Eval epoch: 5, loss: 394.51, binary cross-entropy: 388.54, KL divergence: 5.97\n",
      "Eval epoch: 6, loss: 375.41, binary cross-entropy: 367.59, KL divergence: 7.83\n",
      "Eval epoch: 7, loss: 365.03, binary cross-entropy: 356.02, KL divergence: 9.01\n",
      "Eval epoch: 8, loss: 360.92, binary cross-entropy: 351.43, KL divergence: 9.49\n",
      "Eval epoch: 9, loss: 360.18, binary cross-entropy: 350.60, KL divergence: 9.58\n",
      "Eval epoch: 10, loss: 360.18, binary cross-entropy: 350.60, KL divergence: 9.58\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  for _ in range(steps_per_epoch):\n",
    "    batch = next(train_ds)  \n",
    "    # Use a separate PRNG key to permute image data during shuffling.\n",
    "    rng, key = jax.random.split(key=key)\n",
    "      # Run an optimization step over a training batch.\n",
    "    state = train_step(state=state, batch=batch, z_rng=key)\n",
    "\n",
    "  metrics, comparison, sample = eval_step(params=state.params, images=test_ds, z=z, z_rng=eval_rng)\n",
    "  print('Eval epoch: %d, loss: %.2f, binary cross-entropy: %.2f, KL divergence: %.2f' % (\n",
    "      epoch+1, metrics['loss'], metrics['bce'], metrics['kld']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "vae_tutorial.ipynb",
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
